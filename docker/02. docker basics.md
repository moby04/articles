# Docker

## Abstract

Ever since Docker was released in 2013 it has becoming a new industry standard for running applications both in dev as well as
production environments. Docker images are quite simple to build but, as any other powerfull tool, they have a few concepts
that are easy to overlook.

This document aims to describe basic concepts of Docker and suggest a few good practices. It should be considered as a collection
of random hints rather than comprehensive guide.

## Basics

Containers concept is not limited to Docker. In fact there is an open source community under Linux Foundation called
[Open Container Initiative](https://www.opencontainers.org) that takes care over the industry standards since it was started by
Docker Inc. in 2015. As such, there are other implementations available but Docker is the most popular and as such it is the main
focus of this article.

Many concepts described below will apply to other implementations as well but they were tested on Docker only.

### What is Docker

Docker is a broad term that actually can mean something totally different depending on exact context. It can also mean Docker Image
or Container running a specific application workload but this term is also used to refer to the actual applications responsible for
running these workloads. Finally Docker might refer (though it's the least commonly used meaning) to a language used to write
`Dockerfile`s to define Images.

It is mentioned in previous paragraph that there are multiple applications responsible for running Containers. These are:

1. Docker Engine (or Daemon). It is a process that manages various mechanisms and features of OS to ensure that Containers are running
   on the host operating system in a fully separated environment.
2. Docker CLI. A command line client that communicates with Docker Daemon.

Usually both these applications run on the same machine but that does not have to be the case every time. In certain situation it
is possible to configure Docker CLI so that it will communicate with a Daemon running on a different machine - both virtual and
physical.

Besides purely technical meaning, Docker is also a name of a [company](https://www.docker.com/company) that maintains entire
technology.

### About Docker Daemon

It is important to understand that Deamon actually needs to run as a root to make use of multiple kernel level concepts. This means
that it is Docker's responsibility to restrict permissions of processes running within containers and improperly designed or run
Images can be a serious threat to the system security.

## Implementation nuances

Exact implementation details underlying containers are way out of scope of this article but some concepts are worth mentioning.
Check [this article](https://www.littleman.co/articles/what-is-a-container/) for more detailed explantion of containerization concepts.

### Images vs Containers

Main terms related to Docker are Images and Containers. Sometimes these terms are being mixed but in fact there's important difference
between them and this should be clear.

Docker Image is a result of executing `docker image build` command. It is actually a definition of a workload and defines the
environment in which given application workload will run. `Dockerfile` is used to define this environment. Every Image consists of
multiple layers - one for each command within `Dockerfile` - that are being cached and reused by Docker Engine. All image layers
are read only.

Docker Container is actual running instance of the workload. If you're a developer you might think of it that in a way Container is
a running instance of an Image in similar way like object is an instance of a specific class. Container adds a writable layer on top
of Image's read-only ones.

This layer contains changes made while container is running and it can be saved as a new read-only layer to create new Docker Image.

### Image Layers

As mentioned above Docker Images consist of multiple layers that are put on top of previous ones. Thanks to Union File System each
layer can change or add files in the filesystem that will be seen by resulting application. Old files however will be only hidden,
not replaced - original files from previous layers will be still stored in resulting Image. 

Consider following `Dockerfile`:

```dockerfile
FROM busybox
COPY file1.txt /test.txt
COPY file2.txt /test.txt
```

This image creates a simple image and stores some text file within. We can check the contents by building and running it:

```bash
╰⎼$ cat file1.txt
foo

╰⎼$ cat file2.txt
bar

$ docker image build -t layer .
Sending build context to Docker daemon  4.096kB
Step 1/3 : FROM busybox
 ---> b534869c81f0
Step 2/3 : COPY file1.txt /test.txt
 ---> fabb19e654b4
Step 3/3 : COPY file2.txt /test.txt
 ---> 290bfaf6a0e0
Successfully built 290bfaf6a0e0
Successfully tagged layer:latest

╰⎼$ docker container run -it --rm layer sh
/ # cat /test.txt
bar
/ #
```

As you can see from container perspective `/test.txt` contains contents of `file2.txt` from host system. This does not mean however
that `file1.txt` does not exist in the resulting image. Docker adds a new layer for each command in `Dockerfile` and any file changes
are reflected by mechanics of UFS - only the latest one will be actually visible.

We can see how exactly the Image is constructed by checking its history:

```bash
$ docker image history layer:latest 
IMAGE               CREATED             CREATED BY                                      SIZE                COMMENT
290bfaf6a0e0        2 minutes ago       /bin/sh -c #(nop) COPY file:3f44886ae05277f8…   4B                  
fabb19e654b4        2 minutes ago       /bin/sh -c #(nop) COPY file:7c3bda9dece79876…   4B                  
b534869c81f0        3 days ago          /bin/sh -c #(nop)  CMD ["sh"]                   0B                  
<missing>           3 days ago          /bin/sh -c #(nop) ADD file:884f543fc51111835…   1.22MB
```

Above result says that original image was rebuilt 3 days ago and default command was specified as `sh`. This part was done within
`busybox` definition. On top of that there were two layers added by our example `Dockerfile` adding a simple text file. Layer
`fabb19e654b4` contains our `file1.txt` and then layer `290bfaf6a0e0` added `file2.txt` in the same location.

See [Dive](https://github.com/wagoodman/dive) tool if you want to browse actual contents of any Docker image. 

This approach allows Docker to cache and reuse parts of the data in multiple Images. 

### Cache busting

Docker will try to reuse existing layers as much as possible. However, when some command in Dockerfile changes or files that need
to be added to the container by `COPY`/`ADD` command change, this and following commands will result with overal image diverging:

```bash
╭tkaplonski@tkaplonski-XPS-15-9560:~/Documents/articles/examples/cache-busting(master)
╰⎼$ ll
total 16
drwxr-xr-x 4 tkaplonski tkaplonski 4096 Dec  2 09:09 .
drwxr-xr-x 3 tkaplonski tkaplonski 4096 Dec  2 09:09 ..
drwxr-xr-x 2 tkaplonski tkaplonski 4096 Dec  2 09:05 v1
drwxr-xr-x 2 tkaplonski tkaplonski 4096 Dec  2 09:06 v2
╭tkaplonski@tkaplonski-XPS-15-9560:~/Documents/articles/examples/cache-busting(master)
╰⎼$ cat v1/Dockerfile 
FROM busybox
RUN echo "layer 1"
RUN echo "layer 2"
RUN echo "layer 3"
╭tkaplonski@tkaplonski-XPS-15-9560:~/Documents/articles/examples/cache-busting(master)
╰⎼$ cat v2/Dockerfile 
FROM busybox
RUN echo "layer 1"
RUN echo "layer 2 changed!"
RUN echo "layer 3"
╭tkaplonski@tkaplonski-XPS-15-9560:~/Documents/articles/examples/cache-busting(master)
╰⎼$ docker image build -t cache-busting:v1 ./v1/
Sending build context to Docker daemon  2.048kB
Step 1/4 : FROM busybox
 ---> 020584afccce
Step 2/4 : RUN echo "layer 1"
 ---> Running in 4e5ffca25a65
layer 1
Removing intermediate container 4e5ffca25a65
 ---> 198f4fab9455
Step 3/4 : RUN echo "layer 2"
 ---> Running in 02086b886409
layer 2
Removing intermediate container 02086b886409
 ---> d1d7f8980ae4
Step 4/4 : RUN echo "layer 3"
 ---> Running in 229547a0957b
layer 3
Removing intermediate container 229547a0957b
 ---> 287c15a927e4
Successfully built 287c15a927e4
Successfully tagged cache-busting:v1
╭tkaplonski@tkaplonski-XPS-15-9560:~/Documents/articles/examples/cache-busting(master)
╰⎼$ docker image build -t cache-busting:v2 ./v2/
Sending build context to Docker daemon  2.048kB
Step 1/4 : FROM busybox
 ---> 020584afccce
Step 2/4 : RUN echo "layer 1"
 ---> Using cache
 ---> 198f4fab9455
Step 3/4 : RUN echo "layer 2 changed!"
 ---> Running in 692b7a4f394e
layer 2 changed!
Removing intermediate container 692b7a4f394e
 ---> e91e58bdca83
Step 4/4 : RUN echo "layer 3"
 ---> Running in a13a8ca18ce9
layer 3
Removing intermediate container a13a8ca18ce9
 ---> 63f754530481
Successfully built 63f754530481
Successfully tagged cache-busting:v2
```

As you can see above Docker did not execute `echo "layer 1"` while building second version of the image and just used the result from
the cache, This might result with some side effects and some will be mentioned later in this text.

On the other hand you can see that even though last command does not differ between versions it gets executed again. The reason for
this is that entire cache got busted at previous step and all the following commands need to be executed again.

### Containers vs Virtual Machines

Both containers and VMs enable to encapsulate given workload from other processes running on the same hardware. They are not the same
and it is important to understand the difference.

Virtual Machine is a full emulated computer running within host system. Through virtualization features it creates entire environment
with separate CPU, memory and other hardware resources like graphic card. Within such virtualized hardware we can run any operating
system we want. This virtualization layer provides much higher level of independence from host's hardware and OS but it comes for
a price of high resource requirments used for the virtualization only.

Containers on the other hand are just isolated branches of host operating system process and filesystem trees. At the end of the day
they use the same OS kernel and underlying hardware as host environment. This means that developer is limited to a single operating
system (usually Linux but there are also Windows based containers nowadays) but can differentiate all the packages used within
container (even different Linux distribution). In practical perspective resource cost of running container is much smaller than
Virtual Machine.

As menetioned all the processes running in the host operating system which means they are visible in host's process list but with
different PID:

```bash
╭tkaplonski@tkaplonski-XPS-15-9560:~
╰⎼$ docker container run -d --rm busybox sh -c "while [ 1 ] ; do echo "foo"; sleep 5; done;"
784d5d22439063ca1eef2b230f6581063a94658b0ae48ebe5e89ef927e06dd2e
╭tkaplonski@tkaplonski-XPS-15-9560:~
╰⎼$ docker container ls
CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
784d5d224390        busybox             "sh -c 'while [ 1 ] …"   4 seconds ago       Up 3 seconds                            dreamy_boyd
╭tkaplonski@tkaplonski-XPS-15-9560:~
╰⎼$ docker container exec -it dreamy_boyd sh
/ # ps aux
PID   USER     TIME  COMMAND
    1 root      0:00 sh -c while [ 1 ] ; do echo foo; sleep 5; done;
    9 root      0:00 sh
   14 root      0:00 sleep 5
   15 root      0:00 ps aux
/ # exit
╭tkaplonski@tkaplonski-XPS-15-9560:~
╰⎼$ ps aux | grep "sh -c while"
root      9545  0.1  0.0   1300     4 ?        Ss   08:02   0:00 sh -c while [ 1 ] ; do echo foo; sleep 5; done;
tkaplon+  9778  0.0  0.0   6208   892 pts/1    S+   08:03   0:00 grep --color=auto sh -c while
```

### Docker under various operating systems

As mentioned above, most Docker Images are Linux based and as such they require Linux kernel to run. For this reason Docker on Linux
is "just" another process. In case of other operating systems however like OSX and Windows it requires virtual machine running to run
Docker Daemon on Linux kernel. There are official applications that handle this virtualization (Docker for Mac and Docker for Windows)
so that the behavior is almost seemless. However there is certain performance taxation on those operating systems.

Another thing worth mentioning is that Docker for Windows requires Windows 10 Professional because it uses HyperV, In case you are
using older version of Windows or Win10 Home you can alternatively use Docker Toolbox that uses Oracle's VirtualBox for running Docker
Daemon.

### Using Docker to test ARM applications

Docker can be used to run applications designed for other architectures. Most important application is testing ARM applications on
desktop computers. There can be number of reasons for that: testing mobile applications, testing IoT/Raspberry PI apps or... testing
application before running it in some cloud based machines (for instance AWS offers `A` EC2 instance family). In fact Docker
[announced official partnership with ARM](https://www.docker.com/docker-news-and-press/docker-arm-partner-frictionless-cloud-native-software-model-cloud-edge-iot) on April 24th 2019.

From technical perspective this requires emulation. In case of Windows or MacOS it is provided out of the box by Docker Desktop but on
Linux there's no Virtual Machine by default. To overcome this you need to use QEMU which will emulate different architecture. 

For exact information how to do this please check any of following articles:

- <https://www.docker.com/blog/getting-started-with-docker-for-arm-on-linux/>
- <https://blog.hypriot.com/post/docker-intel-runs-arm-containers/>
- <https://ownyourbits.com/2018/06/27/running-and-building-arm-docker-containers-in-x86/>

### Image naming

Docker Image name consists of a few parts with various meaning. Generally the name can be described as

```bash
[REGISTRY_URL[:PORT]/][NAMESPACE/]IMAGE_NAME[:TAG][@DIGEST]
```

As the format above sugests most parts of the Image are actually optional:

- REGISTRY_URL - this is the host of Docker Registry where the Image is stored. If ommited it defaults to `hub.docker.com` but
  otherwise it can be used to inform Docker Daemon to search for the image in private or public alternatives. Depending on the
  specific Registry additional authentication can be required.
- PORT - Port on which registry listens to connections. It's worth mentioning here that most implementations of Docker Registries are
  containerized themselves which means this port is often mapped to HTTP 80 or HTTPS 443. App inside container usually listens on
  port 5000
- NAMESPACE - Namespaces have particularly important meaning in terms of default Docker Hub. All users and organizations get their
  own namespace that can be used to identify Image owner. Popular tools however often have _official_ images created in cooperation
  between the tool authoring organization and Docker Inc specialists to guarantee their high quality and best practice compliance.These images have empty namespace in `hub.docker.com` (in public url it is represented by single underscore character)
- IMAGE_NAME - the only part of the format that is not optional
- TAG - Images in the registry can contain multiple versions distinguished by tag name. If ommited this part defaults to `latest` but,
  unless you're doing some dev tries, you should always specify exact TAG you want to work with.
- DIGEST - Even though it is not a good practice to change the tag it might sometimes happen. In such cases you can always refer to
  exact build by its digest (or at least part of it that is long enough to identify specific build)

## Ephemeral nature of containers

It is important to understand that running containers are ephemeral by nature. This means that it should be safe to stop them and
create new ones at any point in time without any risk. In fact this is the case in orchestration engines: whenever given container
misbehaves it gets killed and a new one is getting started automatically.

### Volumes

To ensure stateless nature of containers Docker comes with a mechanism of `Volume`s - external storage that can be attached to the
container. By default volume contents is stored in local file system under Docker directory. Exact location depends on the host
operating system but on Linux it would be under `/var/lib/docker/volumes/` - each volume would be a separate directory here.

This default behavior however is just a tip of an ice berg. In fact, Volumes are managed by plugin system that allows to attach
external storages in various environments. This is important in distributed environments where Volume can represent AWS EBS, Azure
Blob Storage, Kubernetes Volume, etc. All these and much more can be mapped to Docker's Volume mechanism so that from perspective of
application running within a container they would become just another directory in the file system.

All that developer needs to worry about is to remember that any data that should be persisted after container gets deleted need to be
stored within locations mounted as external Volumes.

### Volume Types

There are 3 types of volumes that developers should be aware of:

1. Anonymous volume. It gets created if `Dockerfile` defines some directory within an Image as a Volume but the volume is not provided
   when container is booted. In such case the volume will be given a unique hash based name. By default such volume will no be removed
   when container gets deleted but it will also not get automatically reused when new container of given Image gets started. Potential
   side effect of this is that Docker will continue creating new volumes that will never get reused so they will require manual leanup.
2. Named volume. During container creation Volume can get explicit name. In such case Docker Engine will try to find if a Volume with
   this name already exists to attach it to a new container and create a new one only in case it cannot find one. This should be
   considered default choice in most cases.
3. Bind mount. Finally Docker comes with a mechanism of mounting a directory or a single file from host file system into container.
   Changes in the container will be reflected in the host and vic versa. This can be used for mounting files that are under active
   development (see `Application code base as part of the image`) or to enable communication with processes in the host machine.

By default Volumes are mount with read-write permission but it can be changed while Volume is being attached so that the data inside
the volume will be available to the application running in container in read only mode.

### Bind mount for communication with Daemon

Some tools need to be able to communicate directly with the Docker Engine that runs the containerized application. To achieve this
you just need to bind mount `/var/run/docker.sock` to container's files system.

Quite common example of such tool is Traefik - reverse proxy that can routerequests based on L7 of OSI model instead of port only.
Another example could be running some kind of web based dashboard as a GUI for local developer's setup
(i.e. [Portainer](https://www.portainer.io/)).

## Good practices

### Keep app image simillar between environments!

Since containers contain all the dependencies required by given application it is possible to design the Image in a way that they
can be reused in all stages of application lifecycle. Naturally in dev environments you will need set of tools that are not supposed
to be shipped to production but they should be only added on top of default Image instead of creating totally separate image
for different stage.

Simplistic example could be:
```bash
#base-image/Dockerfile
FROM nginx

RUN apt install -y php7.0

CMD ["nginx", "-d", "daemon off"]
#-------------

docker image build -t base-app .

#dev-image/Dockerfile
FROM base-app

RUN apt install -y php7.0-xdebug
#-------------

docker image build -t dev-app .
```

Any runtime differences related to cloud based hosting is responsibility of orchestration layer which will be explained separately.
From application perspective everything should look the same no matter if it's executed on local developer's machine, on testing
server or in distributed cloud environment.

### Application code base as part of the image

Since Docker Image defines entire runtime environment for a given application it should be assumed that the application source code is
actually part of this Image. For this reason source code should be always added into Image's file system so that it can be deployed on
production environments without further operations related to the build.

It might seem that this would be problematic in case of interpreted languages like PHP or NodeJS as it would require rebuilding entire
Image each time developer does a change during his work however there are methods to avoid this issue. Besides mechanism of Volumes
Docker has a mechanism called `Bind Mount` that maps files from host operating system into container itself so that in development
environments it's possible to work within regular IDEs and see implemented changes immediately at the container level.

Volumes and Bind Mounts are being attached during container start, after the Image has been built so these files are actually added
on top of previously created read-only Image layers. This means that even if Image had the application source code baked in this
source will be hidden while running development environment.

### Alpine vs Debian

There are two main distributions used as base for most Docker Images in the community: Alpine and Debian. Most official images by
default use Debian as a preferred flavor but they are available also in Alpine version.

Most important difference is the size: Alpine is minimalistic so images based on it are smaller than those Debian based. In recent
months however basic Debian distribution for containers got greatly reduced and the difference is not as big as it was in 2017 or
earlier (around 100MB vs around 25MB usually).

This minimalizm comes for a price of various nuances that might result with application misbehaving (i.e. Alpine uses buggy flavor of
`awk` that can truncate big numeric values) so you should be very careful while deciding to go with Alpine distribution.

Also, from security perspective it's importatn to know that Alpine has issues with most CVE scanners.

Still, in certain scenarios small size of Alpine based images is a great advantage. One of the biggest advantages of Alpine versions
is the fact their minimalizm - smaller amount of packages shipped by default means smaller attack surface in production images.

### ADD vs COPY

Both `ADD` and `COPY` commands in `Dockerfile` can be used to add certain file or directory to the Image. The difference between them
is that `ADD` has some extra capabilities that can result with side effects. For this reason it is considered best practice to use
`COPY` unless those extra capabilities are needed in particular scenario.

Capabilities of `ADD`:

1. `ADD` can be used to download files from the internet while `COPY` works with local file system only.
2. `ADD` will automatically extract TAR archives and add the result of decompression as a folder within the Image.

### Docker CLI syntax

There are two flavors of Docker CLI commands:

1. Single level commands that existed from the very beginning of Docker
2. Management groups introduced in Docker June 2017 with version 1.13

Since Docker tries to keep backwards compatibility both versions are supported but managemenr groups offer much more capabilities. For
consistency I strongly recommend management group approach, i.e. use `docker container ls` instead of `docker ps` or `docker image rm`
instead of `docker rmi`.

### Order of commands in Dockerfile

Layered composition of Docker Images described above means that Deamon will try to reuse as much cached layers as possible but once
certain Image diverge from existing order all following commands will actually create new layers. For this reason you should try to
put commands that are most likely to change in the future at the end of Dockerfile.

### Single Port for multiple containers

It's common that complex application needs to expose multiple containers on a single port. Example of such situation could be:

1. Magento application running in a container with PHP. This container might need to be available as `api.app.local` for REST API and
   `app.local/admin` for Magento backend access.
2. NodeJS application for modern PWA based frontend. This container might need to be available as default choice for `app.local`
3. MySQL database

In such workload you won't have a problem with MySQL container as it would listen on separate port. The issue however could be that
PHP and NodeJS containers should listen on the same ports (80 and/or 443). Docker by default will not be able to handle this as it can
only expose one port from host to the selected container.

Solution for this would be to introduce another container running a reverse proxy In this kind of setup you expose reverse proxy on
regular HTTP(s) port and configure it to forward the traffik based on L7 information of network OSI model. In Sitewards we usually use
[Traefik](http://www.traefik.io) but Nginx is also popular choice for that.

Usually such reverse proxy requires direct communication with Docker Daemon to dynamically read current setup configuration to adjust
routing rules. Bind Mount mechanism described above can be used for that.

### Webserver as separate container or combined with PHP?

There is a dispute, especially in case of PHP based application, if webserver should be running in the same container as the
interpreter for the actual application. In case of PHP there are two major philosophies supported by most popular web servers:

1. Apache. Official image for PHP runs both web server and and PHP as a single container with subprocess.
2. Nginx. On the other hand official Nginx implementation suggests to run one container for Nginx that would forward requests
   to another container running PHP in FPM mode.

Personally I tend to prefer to run both processes in a single container for a few reasons. Understanding of them however is needed
since there might be some considerations in specific scenario that will make this approach worse choice.

Most of these considerations however are related to the way how the application will be deployed on production workloads and exact
strategies to handle those will be discussed in the article about container orchestration.

#### Communication overhead

Even if all the containers run on the same host machine there is certain cost of communication between containers. Such communication
goes through virtual network created by Docker Engine with its own DNS resolution to point to the workload. This overhead is very
small and could be ignored on single host setup but the cost gets much bigger in production workloads that are often using multiple
machines and actual placement of the container is being dynamically determined by the orchestrator, In such case local communication
between tightly related processes will need to happen over the network instead of localhost communication.

Some orchestration engines offer additional control over the container placement but that is not always the case (i.e. Docker Swarm
does not). And even if it was it greatly complicates overall system complexity.

#### Shared files

Due to historical reasons it's common scenario that both PHP and webserver need to share the same files like graphics, configurations,
etc. This can result with various problems on production workloads if the same volume need to be mounted to various containers.

Quite often the only reasonable solution here would be to download these static resources on container startup which would result with
greatly increased boot time.

#### Nginx role

Nginx has generally way more applications than Apache. Besides working as a webserver it is common case to use it as reverse proxy 
described above) or even load balancer to distribute the traffic between multiple instances of the same containerized application.

This is the reason why Nginx is commonly considered a separate process. In cases, when Nginx actually works as load balancer and/or
reverse proxy it is desirable to be able to manage it indempendently from PHP. However if the sole purpose of Nginx is to handle HTTP
requests for PHP based application it's better to handle both processes together.

#### PHP as non web based application

Not all PHP based workloads are actually web based. Modern applications often require multiple cron jobs or background workers. In
such cases PHP might require scaling indemendent from webserver itself.

### Package managers vs layer caching

#### Outdated package state

One of the side effects of the way how Docker handles multiple layers you should be careful when using package managers within your
dockerfiles. It is crucial to remember that each command within Docker file is a separate layer that will be separately saved and
cached:

```bash
FROM nginx #layer 1
RUN apt update #layer 2
RUN apt install curl #layer 3
```

In above example second layer will get cached and Docker will try to reuse it as much as possible. After some time you might add some
additional package to install:
```bash
FROM nginx #layer 1
RUN apt update #layer 2
RUN apt install curl wget #layer 3b
```

In above scenario Docker will not execute `apt update` again and just use layer created during the first build - in this case outdated
state of package repositories.

#### Package state cache in resulting build

Another caveat of the layer mechanism is that the files that exist in any layer used to build an image are never actually removed by
the next layer. Instead they are just hidden with Union File System. Look at following example:

```bash
FROM nginx #layer 1
RUN apt update #layer 2
RUN apt install curl #layer 3
RUN apt clean #layer 4
```

In this case developer might believe that he cleaned apt cache by running `apt clean`. However though resulting image is built on top
of layer 2 that actually contained entire cache. This cache will not be visible from the container level as it will be overriden in
layer 4 but files will be still there so the image will be bigger.

#### Solution

For these reasons it is important to always include update and cleanup operation in the same `RUN` command as actual package
installation:

```bash
FROM nginx

RUN apt update && \
  apt install curl && \
  apt clean
```

This way Docker will create layer holding the state of image after required packages get installed and package manager's state gets
cleaned up.

## Multistage builds

In mid 2017 Docker introduced a multi-stage build mechanism that allow multiple advanced techniques to optimize the resulting image.

### Tools used for build only

Many applications require additional packages to build them that should not be present in production grade application. As mentioned
above, developer should always work on environment that resembles production as much as possible so his/her images should be always
designed for production in fact.

Let's take a look at following Dockerfile

```bash
FROM composer as build
WORKDIR /src
COPY . .
RUN composer install

FROM php-apache
COPY --from=build /src /var/www/html
```

Each `FROM` statement here actually starts a stage of the build that does not need to depend on the other ones. In this particular
scenario we used basic Composer image to download all required vendor packages in temporary Image and then only copied the result to
target branch.

Resulting image after the build will not contain Composer at all - not only in resulting file system visible to the application but
also in any layer used to achieve this state before.

### Dev differences in a single Dockerfile

Every stage of the Image build can be named and treated as separate target so multi stage builds can be used to avoid handling
multiple Dockerfiles for various environments that refer each other. So instead of two separate Dockerfile presented in 
`Keep app image simillar between environments!` paragraph we could create just one:

```bash
FROM nginx as prod
RUN apt install -y php7.0
CMD ["nginx", "-d", "daemon off"]

FROM prod as dev
RUN apt install -y php7.0-xdebug
```

Now we could run `docker image build -t base-app --target prod .` to run production and `docker image built -t dev-app --target dev .`
to build development version. In case `--target` is ommitted Docker will build all stages and result will be effect of the last one so
in this specific case `--target dev` could be safely removed and achieved result would be the same.

### CI testing as build stage

A special case of multi stage build could be running unit tests. They could get executed on each build and not allow pipeline to even
push to registry image in state where the application behaves improper:

```bash
FROM php as temporary-stage
WORKDIR /var/www/html
COPY . .

FROM temporary-stage as test
RUN wget -O phpunit https://phar.phpunit.de/phpunit-8.phar && chmod +x phpunit
RUN ./phpunit --bootstrap src/autoload.php --testdox tests

FROM temporary-stage as prod
```

In such scenario running `docker image build .` would result with executing `test` stage that would stop the following stages if some tests do not pass.

## Docker Compose

TODO

## Learning materials

TODO